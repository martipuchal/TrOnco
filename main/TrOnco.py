# For command line arguments
import argparse
import os
from pathlib import Path
import sys
from datetime import date
import polars as pl
import numpy as np


import xgboost as xgb
import joblib
from Bio.Seq import Seq
import pyfaidx
import tensorflow as tf

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'



# Annotation tracks were generated by UCSC allowed to perform the analysis.
Atrack_DEFAULT = "hg19"
Atrack_ALLOWED = ["hg18", "hg19", "hg38"] #! S'ha de mirar d'incloure la versiÃ³ nova T2TCHM13v2.0(hs1)

# Initialize parser
parser = argparse.ArgumentParser(
    prog='TrOnco.py',
    usage="TrOnco.py input_file output_file [options]\n" +
                "-t -> Supported tissue types: " +
                "EPI, HEM, MES, END or AVG \n" +
                "Version 1.0, 29Jul2025\n",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
)

# Adding optional argument

## Annotation
parser.add_argument("-a", 
                    help = "Select Annotation tracks generated by UCSC",
                    default=Atrack_DEFAULT, choices=Atrack_ALLOWED
                    )

# Deactivate gene anotation
parser.add_argument('-g', action='store_true', help="Deactivate gene annotation and use pre-existing gene annotation. Gene column need to be called geneName+[5 or 3] (geneName5/geneName3)")

parser.add_argument("-v", action='store_true', help="Save all the information used in the Fusion analysis.")


# Path to file
parser.add_argument("inputFileName", help= "Path and file name of the input")

# input type (deprecated)
#parser.add_argument("inputType", help= "Input type (coord, fcatcher, tophat, etc.)")

# Tissue type
parser.add_argument("-t","--tissue", nargs="?", default="-",
                        help="Tissue type (Epithelial: EPI, Hematological:HEM, Mesenchymal: MES, Endometrial: END, Not defined(Averege): AVG) or '-' (default: -)")

# Path to the output file
parser.add_argument("outputFileName", help= "Path and fiel name to the output")


args = parser.parse_args()


# Check library state
Homedir = str(Path(__file__).resolve().parent.parent)

Homedir = Homedir+"/resources"

canonicalTranscriptsFileName = Homedir+"/common/refseq_canonical.txt"
refseqFileName = Homedir+f"/common/refseq_full_{args.a}.txt" #! S'hauria d'afegir el fitxer amb la nova llibreria de T2T
libFolderName = Homedir+"/libs"
domainsFileName = Homedir+"/common/domains.txt"; piisFileName = Homedir+"/common/piis.txt"
gene2DavidIdFileName = Homedir+"/common/gene_symbol2id.txt"
ffasRangesFileName = Homedir+"/common/ffas_range.txt"
go2themeFileName = Homedir+"/common/go2theme.txt"
classifierFileName = Homedir+"/common/modelRF.bin"
gene2GO_Name = Homedir+"/common/biomart_results"; genomeFileName = Homedir+f"/common/{args.a}.fa"
XGBclassifierFIleName = Homedir+"/common/modelXGB.json"; TFclassifierFIleName  = Homedir+"/common/modelTF.h5"
configFIleName = Homedir+"/common/config.txt"


All_files = [canonicalTranscriptsFileName, refseqFileName, libFolderName,
             domainsFileName, piisFileName, gene2DavidIdFileName, 
             ffasRangesFileName, go2themeFileName, gene2GO_Name,
             classifierFileName,genomeFileName,XGBclassifierFIleName,
             TFclassifierFIleName,configFIleName]
# Inform of missing libs
missing = [file for file in All_files if not Path(file).exists()]
if len(missing)> 0:
    print("[ERROR] The following resources are missing:\n")
    print("\n".join(missing))
    sys.exit()


# Check tissue type
libs = [dire.upper() for dire in os.listdir(Path(libFolderName)) if Path(libFolderName,dire).is_dir() and os.fspath(Path(libFolderName,dire))  ]
allowedTissueTypes = libs.copy()

if not args.tissue in allowedTissueTypes and args.tissue != "-":
    print( f"[ERROR] Unrecognized tissue type, {args.tissue}. ")
    print("Allowed tissue types are: ",", ".join(allowedTissueTypes))
    sys.exit()


# LOADING BKPT POSITIONS AND MAPPING

print(f"{date.today()} Loading RefSeq data and genome sequence, assuming {args.a}")

genome = pyfaidx.Fasta(genomeFileName)

# Data for mapping
# Refseq canonical data schema
# 0        1       2       3       4       5           6       7           8           9
# #name	chrom	strand	txStart	txEnd	cdsStart	cdsEnd	exonStarts	exonEnds	name2

# Load the canonicla transcripts names file.
with open(canonicalTranscriptsFileName, "r") as canonicalTranscripts_file:
    canonicalTranscripts = [line.rstrip('\n') for line in canonicalTranscripts_file]

def convertStrand(strand:str):
    """Converts the strand annotation to a value

    :param strand: must be + and f for forward adn - or r for reverse, the rest of annotations are going to take as not defined
    :return: A numeric integrer for each input
    """
     
    strand = strand.lower()
    if strand in {"+","f"}:
        return 1
    elif strand in {"-","r"}:
        return -1
    else:
        return 0
    

# Load refseq data of the correspongig genome version. 
refseqFileName_df = pl.read_csv(refseqFileName,separator="\t")
# Filter for only conserve the canonical transcripts.
refseqFileName_df = refseqFileName_df.filter(
    pl.col("#name").is_in(canonicalTranscripts))
# Change the strand characetrs to values using converStrand function.
refseqFileName_df = refseqFileName_df.with_columns(
   [ pl.col("strand").map_elements(convertStrand,return_dtype=int)]
)

#print(f"{date.today()} Reading input file, assuming {args.inputType} format")

# File type optionality, 
#inputTypeArgs = args.inputType.upper().split("-")
#inputType = inputTypeArgs[0]

#minSpan = 1
#minSum = 2

#if len(inputTypeArgs) == 3:
#    minSpan = int(inputTypeArgs[1])
#    minSum = int(inputTypeArgs[2])

# Load input file. Input file must be a tab separated file with: (the col names must concide, not the order.)

# Mandatory columns
# chrom5    pos5    chrom3     pos3    strand5     strand3

# Optional cols (both genes specified or no genes. One one is not acceptable)
# tissue    geneName5   geneName3

inputFile = pl.read_csv(args.inputFileName,separator = "\t")
# Generate a ID for each row for posterior comparison.
inputFile = inputFile.insert_column(0,pl.Series(range(inputFile.height)).alias("Fusion_id"))
# Convert the strand to int
inputFile = inputFile.with_columns(
   [ pl.col("strand5").map_elements(convertStrand,return_dtype=int),
    pl.col("strand3").map_elements(convertStrand,return_dtype=int)]
)
# Generate a col for the tissue with the parameter if not in the input file
if args.tissue != "-":
    if not "tissue" in inputFile.columns:
        print(f"{date.today()} No column called tissue detected, is going to be generated.")
    inputFile = inputFile.with_columns(
        tissue = pl.lit(args.tissue)
    )

#inputFile = inputFile.head(100)

# -g parameter allow us to not annotate fusion with the chromosomal position adn use a pre annotated column
if args.g: # When called -g paramenter beacome TRUE (bool)
    # With the refseq file we obtain exon information for all the genes in the fusions.
    annotated_5_df = inputFile.join(
        refseqFileName_df,
        right_on="name2",
        left_on="geneName5",
        how="left"
    ).select("Fusion_id","chrom5","pos5","tissue","strand5","geneName5","exonStarts","exonEnds","cdsStart","cdsEnd")
    annotated_5_df=annotated_5_df.with_columns(
        pl.col("geneName5").alias("name2")
    )
    annotated_3_df = inputFile.join(
        refseqFileName_df,
        right_on="name2",
        left_on="geneName3",
        how="left"
    ).select("Fusion_id","chrom3","pos3","tissue","strand3","geneName3","exonStarts","exonEnds","cdsStart","cdsEnd")
    annotated_3_df=annotated_3_df.with_columns(
        pl.col("geneName3").alias("name2"))

else:
    # without callig the -g parameter we are going to use the chrom and the pos to find the genes present on this location and generate a row for each gene find in this position. And retreave the exon information
    annotated_5_df = inputFile.join(
        refseqFileName_df,
        right_on="chrom",
        left_on="chrom5",
        how="left"
    ).filter((pl.col("cdsStart")<=pl.col("pos5")) & (pl.col("pos5")<=pl.col("cdsEnd"))).select("Fusion_id","chrom5","pos5","name2","tissue","strand5","exonStarts","exonEnds","cdsStart","cdsEnd")

    annotated_3_df = inputFile.join(
        refseqFileName_df,
        right_on="chrom",
        left_on="chrom3",
        how="left"
    ).filter((pl.col("cdsStart")<=pl.col("pos3")) & (pl.col("pos3")<=pl.col("cdsEnd"))).select("Fusion_id","chrom3","pos3","name2","tissue","strand3","exonStarts","exonEnds","cdsStart","cdsEnd")

def to_protein(df:pl.DataFrame,char_bool:bool):

    """This function generate a dataFrame for 5' and 3' fusion part. In this dataFrame we are going to add protein information. 

    :param df: A polars dataFrame is going to be introduced to iterate through the rows. We are going to use the annotated input dataFrame.
    :param char_bool: A boolean is going to be use to retreave the infroamtion of the 5' part or the 3'. TRUE > 5', FALSE > 3'
    :return: We are going to modify implace the input dataFrame. Moreover, 3 different cols are going to be added in the dataFrame. Fisrt col is goint to if the fusion is in frame, second is going to be the protein
             sequence and thild is going to have the number of aa, the aa lenght.
    """

    char = "5" if char_bool else "3"
    prot_list = []
    inframe_list = []
    aa_len = []
    sequence_list = []
    for row in df.iter_rows(named=True):
        
        cdsStart = row["cdsStart"]
        cdsEnd = row["cdsEnd"]
        chr = row[f'chrom{char}']
        coord = row[f'pos{char}']
        strand = row[f'strand{char}']
        exonStars =[int(values) for values in row["exonStarts"].split(",") if len(values)!=0] if row["exonStarts"] else 0
        
        exonEnds =[int(values) for values in row["exonEnds"].split(",")if len(values)!=0] if row["exonEnds"] else 0

        transcripts_df = pl.DataFrame({"exonStarts":exonStars,"exonEnds":exonEnds},schema=[("exonStarts",pl.Int64),("exonEnds",pl.Int64)])
        val = transcripts_df.filter((pl.col("exonStarts")<=coord)&(pl.col("exonEnds")<=coord)).height != 0 if char_bool else transcripts_df.filter((pl.col("exonStarts")>=coord)&(pl.col("exonEnds")>=coord)).height != 0
        #print(transcripts_df)
        if val:
            
            def to_sequence (exon_start:int,exon_end:int):

                """The exon start and end position we obtain the nucleotide sequence

                :param exon_start: Integrer value corresponding to the start position of the exon.
                :param exon_end : Integrer value corresponding to the final position of the exon.
                :return: A tupple with 2 positions. First a list with the efective start of the exon and the efective end, tanking to acount the cdsStart and cdsEnd. And the exon sequence delimited for the input positions.
                """

                if not (exon_start or exon_end):
                    return "Empty"
                # Differenciate the sequence obtantion process for 5'/3' fusion parts and for reverse/forward genes
                if char_bool: 
                    
                    if strand == -1:
                        effective_start = max(exon_start,coord-1,cdsStart)
                        effective_end = min(exon_end, cdsEnd)
                        if effective_start < effective_end:

                            sequence = genome[chr][effective_start:effective_end].seq
                            return ([effective_start, effective_end],sequence)
                        else:
                            return "Empty"
                    else:
                        effective_start = max(exon_start,cdsStart)
                        effective_end = min(exon_end, coord-1,cdsEnd)
                        if effective_start < effective_end:

                            sequence = genome[chr][effective_start:effective_end].seq
                            return ([effective_start, effective_end],sequence)
                        else:
                            return ("Empty")

                else:
                    if strand == -1:
                        effective_start = max(exon_start,cdsStart)
                        effective_end = min(exon_end, coord-1,cdsEnd)
                        if effective_start < effective_end:

                            sequence = genome[chr][effective_start:effective_end].seq
                            return ([effective_start, effective_end],sequence)
                        else:
                            return ("Empty")
                    
                    else:
                        effective_start = max(exon_start,coord-1,cdsStart)
                        effective_end = min(exon_end, cdsEnd)
                        if effective_start < effective_end:

                            sequence = genome[chr][effective_start:effective_end].seq
                            return ([effective_start, effective_end],sequence)
                        else:
                            return "Empty"
                    
            coords = []
            sequence = ""
            for trans_row in transcripts_df.iter_rows(named=True):
                res = to_sequence(trans_row["exonStarts"],trans_row["exonEnds"])
                if res != "Empty":
                    s_coord, s_sequence = res
                    
                    coords.append(s_coord)
                    sequence += s_sequence
            
            inFrame = "YES" if len(sequence)%3 ==0 else "NO" # repassar com mirar el frame
            inframe_list.append(inFrame)
            # Retrave the protein sequence with the nucleotide sequence.
            if strand == -1:
                coding_seq = str(Seq(sequence).reverse_complement())
                coding_dna = coding_seq[:-1] if char_bool else coding_seq[1:]
                #prot = str(Seq(coding_dna).translate())

            else:
                coding_seq = sequence[:-2] if char_bool else sequence[2:]
                coding_dna = Seq(coding_seq)
                if strand == 0:
                    print("############### Warning no defined strand ###############")
                    print(f'Gene: {row["name2"]} with strand: {strand}. Forward taken as default')
                #prot = str(coding_dna.translate())

            #prot_list.append(prot)
            aa_len.append(len(coding_dna)//3)
            sequence_list.append(str(coding_dna).upper())

        else:
            # No protein generated
            #prot_list.append("NO")
            inframe_list.append("NO")
            aa_len.append(0)
            sequence_list.append("*")
    

    # Arrenge results as a Series object 
    infrmae_series = pl.Series(f"inFrame_{char}",inframe_list)
    #prot_series= pl.Series(f"Protein{char}",prot_list,strict=False)
    aa_len_series = pl.Series(f"aapos{char}",aa_len)
    sequence_series=pl.Series(f"Sequence{char}",sequence_list)


    # Actualize the dataFrame
    #df.insert_column(-1, prot_series)
    df.insert_column(-1, infrmae_series)
    df.insert_column(-1, aa_len_series)
    df.insert_column(-1, sequence_series)
    


# Apply the fucntions     
to_protein(annotated_5_df,True)
to_protein(annotated_3_df,False)

# Expression data
print(f"[{date.today()}] Loading expression-related data")

# Inicializate the DataFrame to store the tissue specific libs data
promData = pl.DataFrame()
exprData = pl.DataFrame()
utrData = pl.DataFrame()

genes=[]

def Cleandf(df:pl.DataFrame,tissue:str):

    """Take the promDataFile/exprDataFile/utrDataFile and rename with the tissue name, fill nuls and nans and set the elements to floats. FInaly remove the extra columns.

    :param df: DataFrame to curate. Have 3 cols, first for the gene names, second for the desired expresion value and thrid for an extra expresion value. 
    :param tissue: Tissue of interest, this param is a string which will name the column in the promDataFile/exprDataFile/utrDataFile.
    :return: A dataFrame with a column_1 with the gene name and a column with the tissue expression.
    """
    
    df = (
    df.with_columns(
        pl.col("column_2").fill_null(0)  
          .cast(pl.Utf8)          
          .str.replace("NaN", "0") 
          .cast(pl.Float64)          
          .alias(tissue)
        )
    )
    df = df.drop("column_2")
    df = df.drop("column_3")
    
    return df

for tissue in libs:

    # Read the tissue specific data
    promDataFile = pl.read_csv(Homedir+"/libs/"+tissue+"/prom.txt",separator="\t",has_header=False,)
    exprDatafile = pl.read_csv(Homedir+"/libs/"+tissue+"/expr.txt",separator="\t",has_header=False,)
    utrDataFile = pl.read_csv(Homedir+"/libs/"+tissue+"/utr.txt",separator="\t",has_header=False,)

    if promData.is_empty():
        promData = Cleandf(promDataFile,tissue)
    else:
        # Generate a dataFrame with the first column with the gene name and for each tissue a column with the prom expression.
        promData = promData.join(
            Cleandf(promDataFile,tissue),
            on="column_1", how="full"
        ).with_columns(pl.col(tissue).fill_null(0))
        promData = promData.drop("column_1_right")
    
    if exprData.is_empty():
        exprData = Cleandf(exprDatafile,tissue)
    else:
        exprData = exprData.join(
            Cleandf(exprDatafile,tissue),
            on="column_1",how="full"
        ).with_columns(pl.col(tissue).fill_null(0))
        exprData = exprData.drop("column_1_right")
        

    if utrData.is_empty():
        utrData = utrDataFile.rename({"column_2": tissue})
    else:
        utrData = utrData.join(
            utrDataFile.rename({"column_2": tissue}),
            on="column_1",how="full"
        ).with_columns(pl.col(tissue).fill_null(0))
        utrData = utrData.drop("column_1_right")
        
# Rename the column
promData = promData.rename({"column_1":"name2"})
exprData = exprData.rename({"column_1":"name2"})
utrData = utrData.rename({"column_1":"name2"})
# Curate df, null remove
promData = promData.filter(pl.col("name2").is_not_null())
exprData = exprData.filter(pl.col("name2").is_not_null())
exprData = exprData.filter(pl.col("name2").is_not_null())



# Domain and PII data
print(f"[{date.today()}] Loading domain and protein interaction interface-related data")

domain_df = pl.read_csv(domainsFileName,has_header=False,separator="\t").rename({"column_1":"geneName","column_2":"featureId","column_3":"aaFrom","column_4":"aaTo","column_5":"Domain/Family"})
piis_df = pl.read_csv(piisFileName,has_header=False,separator="\t").rename({"column_1":"geneName","column_2":"featureId","column_3":"aaFrom","column_4":"aaTo"})

print(f"[{date.today()}] Loading gene ontology data")

gene2GO_df = pl.read_parquet(gene2GO_Name)

#python scripts/main_vector.py ../pau_test/pau_coords.txt manual AVG ../pau_test/pau_fusions.txt,"exonStarts","exonEnds"

# FFAS: theme ids and ranges 
with open(ffasRangesFileName) as f:
    ffas_ranges_raw = [line.strip() for line in f.readlines()] 
theme2IdMap = {}
themeNames = []
for n, theme in enumerate(ffas_ranges_raw[0].split("\t")):
    theme2IdMap[theme] = n
    themeNames.append(theme)
nThemes = len(themeNames)

go_theme_df = pl.read_csv(go2themeFileName,has_header=False,separator="\t").rename({"column_1":"GO","column_2":"Theme"})

# Generate a dataFrame with GO, theme names, theme id
go_theme_df = go_theme_df.with_columns(
    pl.col("Theme").replace(theme2IdMap).alias("IdTheme")
)

# Annotate FPG parts
print(f"[{date.today()}] == Annotating FPG parts")

def get_rows(fpg_df:pl.DataFrame,fivePrime:bool):

    """Parent fucntion to iterate throught the rows of the dataFrame and annotate the domains and pii lost/retained/broken for the fused protein.

    :param fpg_df: A dataFrame is needed to iterate throught the different rows. Is going to be modified inplace.
    :param fivePrime: A bool element is used to differenciate the 5' and 3' dataFrames 
    :return: No object is return, insted we modify the input dataFrame.
    """
        
    strand = "5" if fivePrime else "3"

    domain_retained_list = []; domain_broken_list = []; domain_lost_list = []
    pii_retained_list = []; pii_lost_list = []

    for row in fpg_df.iter_rows(named=True):

        def select_rows(gene:str,sup_data:pl.DataFrame):

            """Child fucntion to perform the analysis in the different rows. The fucntion compares the length of the protein and aa start and end of the domain or the pii.

            :param gene: String with the gene is being analyzed. Is going to be used to finds the gene of interest in the suplementary dataFrame.
            :param sub_data: Suplementary dataFrames containing the domains or the pii data. The oject is a plolars dataFrame.
            :return: A tupple with the retained list, lost list and broken list domain or pii.
            """

            if not gene in sup_data["geneName"]:
                print( gene," Was omited from the pii analysis, not found on the DataBase")
                return ([],[],[])
            # Retreave suplementary information of the gene of interest
            region_df = sup_data.filter(
                pl.col("geneName")==gene
            )
            # Generate a col as a result of the protein length analysis
            case_expr = pl.when(
                # Case 1 conditions
                (pl.col("aaTo") <= row[f"aapos{strand}"]) & fivePrime
            ).then(1).when(
                # Case 2 conditions
                (pl.col("aaFrom") >= row[f"aapos{strand}"]) & fivePrime
            ).then(-1).when(
                # Case 3 conditions
                (pl.col("aaFrom") >= row[f"aapos{strand}"]) & (fivePrime==False)
            ).then(1).when(
                # Case 4 conditions
                (pl.col("aaTo") <= row[f"aapos{strand}"]) & (fivePrime==False)
            ).then(-1).otherwise(0)

            region_df = region_df.with_columns(case_expr.alias("case"))                                                                         

            # Segregate the list based on the case.
            retained =  region_df.filter(pl.col("case") == 1)["featureId"].to_list()
            broken = region_df.filter(pl.col("case") == 0)["featureId"].to_list()
            lost = region_df.filter(pl.col("case") == -1)["featureId"].to_list()

            # Null correction
            retained = retained if retained else []
            broken = broken if broken else []
            lost = lost if lost else []
            
            return (retained,broken,lost)
        
        domain_retained, domain_broken, doamin_lost = select_rows (row["name2"],domain_df)
        domain_retained_list.append(domain_retained)
        domain_broken_list.append(domain_broken)
        domain_lost_list.append(doamin_lost)

        pii_retained, _ , pii_lost = select_rows(row["name2"],piis_df)
        pii_retained_list.append(pii_retained)
        pii_lost_list.append(pii_lost)

    fpg_df.insert_column(-1,pl.Series(f"domain_retained_{strand}",domain_retained_list,strict=False))
    fpg_df.insert_column(-1,pl.Series(f"domain_broken_{strand}",domain_broken_list,strict=False))
    fpg_df.insert_column(-1,pl.Series(f"domain_lost_{strand}",domain_lost_list,strict=False))

    fpg_df.insert_column(-1,pl.Series(f"pii_retained_{strand}",pii_retained_list,strict=False))
    fpg_df.insert_column(-1,pl.Series(f"pii_lost_{strand}",pii_lost_list,strict=False))

get_rows(annotated_5_df,True)
get_rows(annotated_3_df,False)

print(f"[{date.today()}] == Tissue specific")

def exp_data(df:pl.DataFrame):

    """Add the tissue-especific expresion data(promData/exprData/utrData) in the dataFrame.

    :param df: DataFrame is going to be modify with the desired data. 
    :return: Is going to return the modified dataFrame.
    """
    
    labels = ["promData","exprData","utrData"]
    for i, data in zip(labels,[promData,exprData,utrData]):
        feature_long = data.unpivot(
            index="name2",
            variable_name="tissue",
            value_name = i)

        df = df.join(
            feature_long,
            on=["name2", "tissue"],
            how="left")
        
        df = df.with_columns(
            pl.col(i).fill_null(0.0))
        
    return df
    

tissue_esp_5 = exp_data(annotated_5_df)
tissue_esp_3 = exp_data(annotated_3_df)

print(f"[{date.today()}] == Domain & PII related")

def count_piis(df:pl.DataFrame,strand:str,gene_name_col="name2"):

    """Perform the different recounts of the protein interaction part.

    :param df: The dataFrame which is going to be to perform the calcualtions.
    :param strand: String value to perform the analysis to the different strands.
    :param gene_name_col: a string value to determine the name of the column where the gene names are strored.
    :return: A modifiend dataFrame with the new cols added.
    """

    gene_names = df[gene_name_col].to_list()
    piis_retained = df[f"pii_retained_{strand}"].to_list()

    piis_lost = df[f"pii_lost_{strand}"].to_list()

    n_self_piis_retained = [
        lst.count(gene_names[i]) if lst else 0 
        for i, lst in enumerate(piis_retained)    ]
    
    n_piis_retained = [len(lst) if lst else 0 for lst in piis_retained]
    n_self_piis_lost = [
        lst.count(gene_names[i]) if lst else 0 
        for i, lst in enumerate(piis_lost)    ]
    n_piis_lost = [len(lst) if lst else 0 for lst in piis_lost]

    return df.with_columns(
        n_self_piis_retained=pl.Series(n_self_piis_retained),
        n_piis_retained=pl.Series(n_piis_retained),
        n_self_piis_lost=pl.Series(n_self_piis_lost),
        n_piis_lost=pl.Series(n_piis_lost)
    ).with_columns(
        n_piis_retained=pl.col("n_piis_retained") - pl.col("n_self_piis_retained"),
        n_piis_lost=pl.col("n_piis_lost") - pl.col("n_self_piis_lost")
    )

tissue_esp_5_npii = count_piis(tissue_esp_5,"5")
tissue_esp_3_npii = count_piis(tissue_esp_3,"3")

gene_2_theme = gene2GO_df.join(go_theme_df,on="GO").group_by("geneName").agg([
    pl.col("GO"),
    pl.col("Theme"),
    pl.col("IdTheme")
])

def calculate_domain_profile ( sample_df:pl.DataFrame,strand:str):

    """Add the GO theme data to the each gene of the DataFrame, multiple themes are going to be added or none. Aditionaly the themes were count.

    :param df: DataFrame is going to be modify with the desired data.
    :param strand: A string to differenciate the analysis for 5' and 3'. 
    :return: Is going to return the modified dataFrame.
    """

    filtered_g2t_df = gene_2_theme.filter(
        pl.col("geneName").is_in(sample_df["name2"]))
    for category in themeNames:
        filtered_g2t_df = filtered_g2t_df.with_columns(
            pl.col("Theme").list.count_matches(category).alias(category)
        )

    themed_sample_df = sample_df.join(filtered_g2t_df,
                                      left_on="name2",
                                      right_on="geneName",
                                      how="left")
    
    for category in themeNames:
        themed_sample_df = themed_sample_df.with_columns(
            pl.col(category).fill_null(0)
        )
        
    return themed_sample_df

domprofile_df_5 = calculate_domain_profile(tissue_esp_5_npii,"5")
domprofile_df_3 = calculate_domain_profile(tissue_esp_3_npii,"3")

print(f"[{date.today()}] == Variant selection")

def curate_df (sample_df:pl.DataFrame,strand:str):

    """This function remove additional lines in the df. Only is going to keep one line for fusion.

    :param df: DataFrame is going to be modify with the desired data.
    :param strand: A string to differenciate the analysis for 5' and 3'. 
    :return: Is going to return the modified dataFrame.
    """

    df_result = pl.DataFrame()
    for id in range(max(sample_df["Fusion_id"])):

        interest_rows = sample_df.filter(
            pl.col("Fusion_id") == id)
        
        if interest_rows.height > 1:
            row_list_res = []
            for row in interest_rows.iter_rows(named=True):
                count = 0
                #if row[f"Protein{strand}"] != "NO":
                    #count += 1
                if row["promData"] != 0:
                    count += 1
                if row["CTF"]!=0 or row["G"]!=0 or row["H"]!=0 or row["K"]!=0 or row["P"]!=0 or row["TF"]!=0:
                    count += 1
                
                row_list_res.append(count)

            max_mask = [False]*len(row_list_res)
            max_val = 0
            for i, val in enumerate(row_list_res):
                max_mask[i] = True if max_val<val else False
                if max_val<val:
                    max_val = val              

            filter_series = pl.Series(max_mask)
            new_df = interest_rows.filter(filter_series)

        else:
            new_df = interest_rows
        
        if new_df.height>1:
            new_df = new_df[0,:]
        df_result = pl.concat([df_result, new_df])

    return df_result

curated_domprofile_df_5 = curate_df(domprofile_df_5,"5")
curated_domprofile_df_3 = curate_df(domprofile_df_3,"3")


print(f"[{date.today()}] == Vector building")


vector_list_str = ["promData_5","n_domain_retained_5","n_domain_broken_5","n_domain_lost_5","CTF_5","G_5","H_5",
            "K_5","P_5","TF_5","exprData_5","utrData_5","n_piis_retained_5","n_piis_lost_5",
            "promData_3","n_domain_retained_3","n_domain_broken_3","n_domain_lost_3","CTF_3","G_3","H_3",
            "K_3","P_3","TF_3","exprData_3","utrData_3","n_piis_retained_3","n_piis_lost_3"]


def vec_building(Fusion_row:pl.DataFrame,strand:str):

    """ With a row of the dataframe and the strand is generated the part of the vector corresponding to that fusion part.

    :param Fusion_row: Row of the original DataFrame, this row keep the name and is keep the polar DataFrame structure.
    :param strand: string to correcly select the row of the DataFrame of interest.
    :return: List with all the elements of the vector part. 
    """
    return [Fusion_row[f"promData{strand}"],len(Fusion_row[f"domain_retained{strand}"]),len(Fusion_row[f"domain_broken{strand}"]),
        len(Fusion_row[f"domain_lost{strand}"]),Fusion_row[f"CTF{strand}"],Fusion_row[f"G{strand}"], Fusion_row[f"H{strand}"],
        Fusion_row[f"K{strand}"],Fusion_row[f"P{strand}"],Fusion_row[f"TF{strand}"],Fusion_row[f"exprData{strand}"],
        Fusion_row[f"utrData{strand}"],Fusion_row[f"n_piis_retained{strand}"], Fusion_row[f"n_piis_lost{strand}"]]

def vec_parser (line):

    """ Combine the 2 vectors from the fusion parts in order to generate the final vector

    :param line: Takes as imput a row of the DataFrame containing the 5' part and the 3' part.
    :return: Returns a lists with all the elements of the vector if the vector is not complete is going to be empty.
    """

    Fpart5 = vec_building(line,"")
    Fpart3 = vec_building(line,"3")
    vec = Fpart5+Fpart3
    if len(vec) == len(vector_list_str):
        return (vec)
    else:
        return([None])

Fusion5 = domprofile_df_5.rename({"domain_lost_5":"domain_lost","domain_broken_5":"domain_broken","domain_retained_5":"domain_retained"})
Fusion3 = domprofile_df_3.rename({"domain_lost_3":"domain_lost","domain_broken_3":"domain_broken","domain_retained_3":"domain_retained"})

merged = Fusion5.join(Fusion3,
    on="Fusion_id",
    suffix="3"
)

FusProt_list = []
for row in merged.iter_rows(named=True):
    sequence = row["Sequence5"]+row["Sequence3"]
    sequence = sequence.replace("*","")
    n = len(sequence)
    FusProt_list.append(str(Seq(sequence[:n-n%3]).translate()))

fusprot_series = pl.Series("FUSION_PROTEIN",FusProt_list)    

merged.insert_column(-1, fusprot_series)


vector_list2 = []
scores = []
scores2 = []
scores3 = []
lclassification = []
l2classification = []
l3classification = []

# Load RF classifier
RF_model = joblib.load(classifierFileName)

# Load XGBoost model
XGB_model = xgb.XGBClassifier()
XGB_model.load_model(XGBclassifierFIleName)

# Load TensorFlow model
TF_model = tf.keras.models.load_model(TFclassifierFIleName)

def class_round(num:float, threshold:float):

    """ Rounds the result from the classifier with a threshold.

    :param num: Probability which is going to be rounded.
    :param threshold: Probability is going to be use as a threshold.
    :return: Returns a string classifying the probability.
    """
    if num < threshold:
        return "PASSENGER"
    else:
        return "DRIVER"


print(f"[{date.today()}] == Classifier initialization")

config_df = pl.read_csv(configFIleName,separator="\t")
RF_threshold, XGB_threshold, TF_threshold = config_df["thresholds"]

for df_line in merged.iter_rows(named=True):
    # Vector generation
    vector = vec_parser(df_line)
    vector_list2.append(vector)

    # RF prediction
    prob = RF_model.predict_proba([vector])[:, 1]
    scores.append(prob[0])
    classification = class_round(prob[0], RF_threshold)
    classification = "DRIVER" if prob[0]>RF_threshold else "PASSENGER"
    lclassification.append(classification)

    # XGBoost prediction
    preds = XGB_model.predict_proba([vector])[0][1]
    classification2 = class_round(float(preds),XGB_threshold)
    scores2.append(preds)
    l2classification.append(classification2)

    # TensorFlow prediction
    input_X = np.array(vector).reshape(-1, 28, 1)
    #prob_positive = TF_model.predict(input_X)[0][0]
    prob_positive = TF_model.predict(input_X)[0, 1]
    scores3.append(prob_positive)
    classification3 = "DRIVER" if prob_positive>TF_threshold else "PASSENGER"
    l3classification.append(classification3)


# Save the results
merged_final = merged.with_columns(
    pl.Series("vector",vector_list2,strict=False),
    pl.Series("RF_prob", scores),
    pl.Series("XGB_prob", scores2),
    pl.Series("CNN_prob", scores3),
    pl.Series("RF_Classification", lclassification),
    pl.Series("XGB_Classification", l2classification),
    pl.Series("CNN_Classification", l3classification)
)

merged_final = merged_final.with_columns(FUSION_GENES = pl.concat_str("name2","name23",separator="_"),
                          rightBreakpoint = pl.concat_str("chrom5","pos5","strand5",separator=":"),
                          leftBreakpoint = pl.concat_str("chrom3","pos3","strand3",separator=":"))



if args.v:
   # Save the final df for debugin.
    list_cols = [col for col in merged_final.columns if merged_final[col].dtype == pl.List]
    merged_final_save = merged_final.with_columns(
        pl.col(list_cols).map_elements(
            lambda lst: ",".join(str(x) for x in lst) if lst is not None else None,
            return_dtype=pl.String
        ))
    merged_final_save.write_csv(args.outputFileName)
    print(merged_final_save)
else:
    print(merged_final.columns)
    col_list = ["rightBreakpoint","leftBreakpoint","FUSION_GENES","RF_prob","RF_Classification","XGB_prob","XGB_Classification","CNN_prob","CNN_Classification",
                "FUSION_PROTEIN","GO","GO3","domain_retained","domain_broken","domain_lost","domain_retained3","domain_broken3","domain_lost3"]
    merged_final = merged_final.select(col_list).rename({"GO":"GO5","domain_retained":"domain_retained5","domain_broken":"domain_broken5","domain_lost":"domain_lost5"})
    list_cols = [col for col in merged_final.columns if merged_final[col].dtype == pl.List]
    merged_final_save = merged_final.with_columns(
        pl.col(list_cols).map_elements(
            lambda lst: ",".join(str(x) for x in lst) if lst is not None else None,
            return_dtype=pl.String
        ))
    merged_final_save.write_csv(args.outputFileName)
    print(merged_final_save)

print("RandomForest threshold: ",RF_threshold, "XGBoost threshold: ",XGB_threshold, "Tensor Flow CNN threshold: ", TF_threshold)

